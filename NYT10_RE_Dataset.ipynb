{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file found! Loading now...\n",
      "Number of trainable samples: 172415\n",
      "   fb_mid_e1  fb_mid_e2         e1_name         e2_name relation  \\\n",
      "0  m.01l443l   m.04t_bj    dave_holland  barry_altschul       NA   \n",
      "1  m.01l443l   m.04t_bj    dave_holland  barry_altschul       NA   \n",
      "2   m.04t_bj  m.01l443l  barry_altschul    dave_holland       NA   \n",
      "3   m.04t_bj  m.01l443l  barry_altschul    dave_holland       NA   \n",
      "4   m.0frkwp   m.04mh_g            ruth     little_neck       NA   \n",
      "\n",
      "                                            sentence  \n",
      "0  the occasion was suitably exceptional : a reun...  \n",
      "1  tonight he brings his energies and expertise t...  \n",
      "2  the occasion was suitably exceptional : a reun...  \n",
      "3  tonight he brings his energies and expertise t...  \n",
      "4              shapiro -- ruth of little_neck , ny .  \n"
     ]
    }
   ],
   "source": [
    "class NYT10Dataset(Dataset):\n",
    "    \"\"\"NYT10 dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, sentence_file, relation2id_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentence_file (string): Path to the txt file with annotations.\n",
    "            relation2id_file (string): Path to txt file with mapping between relation and id\n",
    "        \"\"\"\n",
    "        self.clean_and_load_dataset(sentence_file)\n",
    "        \n",
    "        self.relation2id = {}\n",
    "        with open(relation2id_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=' ')\n",
    "            for row in reader:\n",
    "                self.relation2id[row[0]] = int(row[1])\n",
    "\n",
    "    def clean_and_load_dataset(self, sentence_file):\n",
    "        cleaned_sentence_file = sentence_file[:-4] + '_cleaned' + sentence_file[-4:]\n",
    "        if os.path.isfile(cleaned_sentence_file):\n",
    "            print('Cleaned file found! Loading now...')\n",
    "            self.sentences_frame = pd.read_csv(cleaned_sentence_file, sep='\\t', keep_default_na=False)\n",
    "            print('Number of trainable samples:', len(self.sentences_frame))\n",
    "            return\n",
    "\n",
    "        print('No cleaned file found, cleaning now...')\n",
    "\n",
    "        colnames = ['fb_mid_e1', 'fb_mid_e2', 'e1_name', 'e2_name', 'relation', 'sentence', 'end']\n",
    "        self.sentences_frame = pd.read_csv(sentence_file, \n",
    "                                           sep='\\t', \n",
    "                                           names=colnames,\n",
    "                                           keep_default_na=False)\n",
    "        self.sentences_frame.drop('end', axis=1, inplace=True)\n",
    "        print('Number of samples loaded:', len(self.sentences_frame))\n",
    "        \n",
    "        broken_samples = []\n",
    "        for i, (_, _, e1_name, e2_name, _, text) in self.sentences_frame.iterrows():\n",
    "            split_text = text.split(' ')\n",
    "            if e1_name not in split_text or e2_name not in split_text:\n",
    "                broken_samples.append(i)\n",
    "        print('Number of broken samples:', len(broken_samples))\n",
    "\n",
    "        self.sentences_frame.drop(broken_samples, axis=0, inplace=True)\n",
    "        print('Number of trainable samples:', len(self.sentences_frame))\n",
    "        \n",
    "        self.sentences_frame.to_csv(cleaned_sentence_file, sep='\\t', index=False)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.sentences_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, _, e1_name, e2_name, relation, text = self.sentences_frame.iloc[idx]\n",
    "        text = text.split(' ')\n",
    "        e1_index = text.index(e1_name)\n",
    "        e2_index = text.index(e2_name)\n",
    "        lower, upper = sorted((e1_index, e2_index))\n",
    "\n",
    "        c1 = text[:lower]\n",
    "        c2 = text[lower+1:upper]\n",
    "        c3 = text[upper+1:]\n",
    "        return c1, c2, c3, self.relation2id[relation]\n",
    "\n",
    "sentences_dataset = NYT10Dataset('data/test.txt', 'data/relation2id.txt')\n",
    "print(sentences_dataset.sentences_frame.head())\n",
    "# print(sentences_dataset.relation2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['rosemary',\n",
       "  'antonelle',\n",
       "  ',',\n",
       "  'the',\n",
       "  'daughter',\n",
       "  'of',\n",
       "  'teresa',\n",
       "  'l.',\n",
       "  'antonelle',\n",
       "  'and',\n",
       "  'patrick',\n",
       "  'antonelle',\n",
       "  'of'],\n",
       " [','],\n",
       " [',',\n",
       "  'was',\n",
       "  'married',\n",
       "  'yesterday',\n",
       "  'afternoon',\n",
       "  'to',\n",
       "  'lt.',\n",
       "  'thomas',\n",
       "  'joseph',\n",
       "  'quast',\n",
       "  ',',\n",
       "  'a',\n",
       "  'son',\n",
       "  'of',\n",
       "  'peggy',\n",
       "  'b.',\n",
       "  'quast',\n",
       "  'and',\n",
       "  'vice',\n",
       "  'adm.',\n",
       "  'philip',\n",
       "  'm.',\n",
       "  'quast',\n",
       "  'of',\n",
       "  'carmel',\n",
       "  ',',\n",
       "  'calif.',\n",
       "  '.'],\n",
       " 48)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385469"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_dataset.sentences_frame[sentences_dataset.sentences_frame['relation'] == 'NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'weitzman',\n",
       "  'family',\n",
       "  'remember',\n",
       "  'with',\n",
       "  'great',\n",
       "  'affection',\n",
       "  'larry',\n",
       "  'plotkin',\n",
       "  ',',\n",
       "  'and',\n",
       "  'join',\n",
       "  'in',\n",
       "  'sorrow'],\n",
       " ['and',\n",
       "  'the',\n",
       "  'selwyns',\n",
       "  ',',\n",
       "  'and',\n",
       "  'in',\n",
       "  'particular',\n",
       "  ',',\n",
       "  'john',\n",
       "  ',',\n",
       "  'sarah',\n",
       "  'and'],\n",
       " ['.'],\n",
       " 0)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_dataset[55990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sentence in sentences_dataset:\n",
    "    x = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
